general:
  name:
  random_seed: 123
#  random_seed: 234
#  random_seed: 345
#  updates_cycle_count: 4000 # simple
  updates_cycle_count: 5000 # hard, vision
  episodes_per_update: 16
  model_updates_per_cycle: 40
  max_path_slack: 1.5
  gpu_usage: 0.01
#  gpu_usage: 0.1 # vision
  actor_gpu_usage: 0.01
#  actor_gpu_usage: 0.1 # vision
  actor_processes: 9
#  actor_processes: 6 # vision
#  actor_processes:
#  actor_processes: 1
  write_train_summaries: 500
  save_model_every_cycles: 100
#  scenario: 'no_obstacles'
#  scenario: 'simple'
  scenario: 'hard'
#  scenario: 'vision'
#  scenario: 'vision_harder'

openrave_rl:
  action_step_size: 0.025
  segment_validity_step: 0.001
  goal_sensitivity: 0.04
  keep_alive_penalty: 0.01
  truncate_penalty: 0.05

model:
  buffer_size: 1000000
  batch_size: 512
#  batch_size: 512 # vision
  gamma: 0.99
  potential_points: [5, -0.02, 0.035]
#  potential_points: [2, 0., 0.075, 3, 0., 0.085, 4, -0.02, 0.05, 4, 0.005, 0.05, 5, 0.005, 0.035, 5, -0.02, 0.035]
  tau: 0.05
#  tau: 0.01
  random_action_probability: 0.02
#  random_action_probability: 0.2
  random_noise_std: 0.005
#  random_noise_std: 0.05
  use_reward_model: True
#  use_reward_model: False
  alter_episode: 0  # natural reward and original episode
#  alter_episode: 1  # use learned reward with episode truncation
#  alter_episode: 2  # use learned reward without episode truncation
  alter_episode_expert: 0  # natural reward and original episode
#  alter_episode_expert: 1  # use learned reward with episode truncation
#  alter_episode_expert: 2  # use learned reward without episode truncation
  failed_motion_planner_trajectories: 8
#  failed_motion_planner_trajectories: 0

test:
  test_every_cycles: 50
  number_of_episodes: 200

validation:
  number_of_episodes: 1000

hindsight:
#  enable: False
  enable: True
#  type: 'goal'
  type: 'future'
  k: 4
#  score_with_reward_model: True
  score_with_reward_model: False

actor:
  learning_rate: 0.001
#  gradient_limit: 0.0
  gradient_limit: 1.0

action_predictor:
  layers: [200, 200, 200, 200]
  activation: 'elu'
  tanh_preactivation_loss_coefficient: 1.0
#  tanh_preactivation_loss_coefficient: 0.0

critic:
  learning_rate: 0.001
#  learning_rate: 0.01
  gradient_limit: 1.0
#  gradient_limit: 0.0
  layers_before_action: [400, 400, 400]
  layers_after_action: [400, 400, 400, 400]
  activation: 'elu'
#  l2_regularization_coefficient: 0.0
  l2_regularization_coefficient: 0.0000001
#  last_layer_tanh: True
  last_layer_tanh: False

reward:
  activation: 'elu'
  layers: [100, 100, 100, 100]
